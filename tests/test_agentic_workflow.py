"""
Comprehensive tests for the Agentic AI Workflow.
Tests individual nodes, edge routing, error recovery, and full workflow execution.
"""

import asyncio
import json
import os
import tempfile
from typing import Dict
from unittest.mock import Mock, patch

import pytest

# Import the workflow components
from agentic_workflow import (
    WorkflowState,
    WorkflowStep,
    build_agentic_workflow,
    content_grading_agent,
    error_recovery_agent,
    finalize_workflow,
    get_letter_grade,
    grammar_analysis_agent,
    initialize_workflow,
    plagiarism_detection_agent,
    quality_check_agent,
    quality_validation_agent,
    relevance_analysis_agent,
    results_aggregation_agent,
    route_workflow,
    run_agentic_workflow,
    summary_generation_agent,
)


class TestWorkflowComponents:
    """Test individual workflow components."""

    def test_letter_grade_conversion(self):
        """Test letter grade conversion function."""
        assert get_letter_grade(9.5) == "A+"
        assert get_letter_grade(8.7) == "A"
        assert get_letter_grade(8.2) == "A-"
        assert get_letter_grade(7.8) == "B+"
        assert get_letter_grade(7.2) == "B"
        assert get_letter_grade(6.7) == "B-"
        assert get_letter_grade(6.2) == "C+"
        assert get_letter_grade(5.7) == "C"
        assert get_letter_grade(5.2) == "C-"
        assert get_letter_grade(4.5) == "D"
        assert get_letter_grade(3.0) == "F"
        assert get_letter_grade(0.0) == "F"

    def test_initialize_workflow(self):
        """Test workflow initialization."""
        state = WorkflowState(
            content="This is a test assignment with sufficient content to trigger all processing requirements.",
            metadata={
                "name": "Test Student",
                "date": "2025-01-15",
                "class": "10",
                "subject": "English",
            },
            source_text="Renaissance information",
            requires_grammar_check=False,
            requires_plagiarism_check=False,
            requires_relevance_check=False,
            requires_grading=False,
            requires_summary=False,
            grammar_result=None,
            plagiarism_result=None,
            relevance_result=None,
            grading_result=None,
            summary_result=None,
            current_step="",
            completed_steps=[],
            errors=[],
            retry_count=0,
            quality_score=0.0,
            final_results=None,
        )

        result_state = initialize_workflow(state)

        assert result_state["requires_grammar_check"] is True
        assert result_state["requires_plagiarism_check"] is True  # Content length > 100
        assert result_state["requires_relevance_check"] is True  # Has source text
        assert result_state["requires_grading"] is True
        assert result_state["requires_summary"] is True  # Content length > 200
        assert result_state["current_step"] == WorkflowStep.QUALITY_CHECK.value
        assert WorkflowStep.INITIALIZE.value in result_state["completed_steps"]

    def test_quality_check_agent(self):
        """Test quality check agent."""
        state = WorkflowState(
            content="This is a well-structured assignment with introduction, body paragraphs, and conclusion. It includes citations [1] and references (Smith, 2020).",
            metadata={"name": "Test Student"},
            source_text="",
            requires_grammar_check=False,
            requires_plagiarism_check=False,
            requires_relevance_check=False,
            requires_grading=False,
            requires_summary=False,
            grammar_result=None,
            plagiarism_result=None,
            relevance_result=None,
            grading_result=None,
            summary_result=None,
            current_step=WorkflowStep.QUALITY_CHECK.value,
            completed_steps=[WorkflowStep.INITIALIZE.value],
            errors=[],
            retry_count=0,
            quality_score=0.0,
            final_results=None,
        )

        result_state = quality_check_agent(state)

        assert result_state["quality_score"] > 0.0
        assert result_state["current_step"] == WorkflowStep.GRAMMAR_ANALYSIS.value
        assert WorkflowStep.QUALITY_CHECK.value in result_state["completed_steps"]

    def test_route_workflow(self):
        """Test workflow routing logic."""
        state = WorkflowState(
            content="",
            metadata={},
            source_text="",
            requires_grammar_check=False,
            requires_plagiarism_check=False,
            requires_relevance_check=False,
            requires_grading=False,
            requires_summary=False,
            grammar_result=None,
            plagiarism_result=None,
            relevance_result=None,
            grading_result=None,
            summary_result=None,
            current_step=WorkflowStep.GRAMMAR_ANALYSIS.value,
            completed_steps=[],
            errors=[],
            retry_count=0,
            quality_score=0.0,
            final_results=None,
        )

        assert route_workflow(state) == "grammar_analysis"

        state["current_step"] = WorkflowStep.PLAGIARISM_DETECTION.value
        assert route_workflow(state) == "plagiarism_detection"

        state["current_step"] = WorkflowStep.FINALIZE.value
        assert route_workflow(state) == "finalize"

        state["current_step"] = "invalid_step"
        assert route_workflow(state) == "__end__"


class TestWorkflowNodesWithMocks:
    """Test workflow nodes with mocked dependencies."""

    @patch("agentic_workflow.groq_llm")
    def test_grammar_analysis_agent_success(self, mock_llm):
        """Test grammar analysis agent with successful LLM response."""
        # Mock LLM response
        mock_response = Mock()
        mock_response.content = "Grammar analysis shows 3 errors in the text."
        mock_llm.invoke.return_value = mock_response

        state = WorkflowState(
            content="Test content with some grammar errors.",
            metadata={"name": "Test Student"},
            source_text="",
            requires_grammar_check=True,
            requires_plagiarism_check=False,
            requires_relevance_check=False,
            requires_grading=False,
            requires_summary=False,
            grammar_result=None,
            plagiarism_result=None,
            relevance_result=None,
            grading_result=None,
            summary_result=None,
            current_step=WorkflowStep.GRAMMAR_ANALYSIS.value,
            completed_steps=[],
            errors=[],
            retry_count=0,
            quality_score=0.0,
            final_results=None,
        )

        result_state = grammar_analysis_agent(state)

        assert result_state["grammar_result"]["status"] == "success"
        assert result_state["grammar_result"]["error_count"] == 3
        assert result_state["current_step"] == WorkflowStep.PLAGIARISM_DETECTION.value
        assert WorkflowStep.GRAMMAR_ANALYSIS.value in result_state["completed_steps"]

    @patch("agentic_workflow.groq_llm", None)
    def test_grammar_analysis_agent_no_llm(self):
        """Test grammar analysis agent when LLM is not available."""
        state = WorkflowState(
            content="Test content",
            metadata={"name": "Test Student"},
            source_text="",
            requires_grammar_check=True,
            requires_plagiarism_check=False,
            requires_relevance_check=False,
            requires_grading=False,
            requires_summary=False,
            grammar_result=None,
            plagiarism_result=None,
            relevance_result=None,
            grading_result=None,
            summary_result=None,
            current_step=WorkflowStep.GRAMMAR_ANALYSIS.value,
            completed_steps=[],
            errors=[],
            retry_count=0,
            quality_score=0.0,
            final_results=None,
        )

        result_state = grammar_analysis_agent(state)

        assert result_state["grammar_result"]["status"] == "error"
        assert result_state["grammar_result"]["error_count"] == -1
        assert "LLM not available" in result_state["errors"][0]

    @patch("agentic_workflow.groq_llm")
    @patch("os.makedirs")
    @patch("builtins.open")
    def test_plagiarism_detection_agent_success(
        self, mock_open, mock_makedirs, mock_llm
    ):
        """Test plagiarism detection agent with successful operation."""
        # Mock LLM response
        mock_response = Mock()
        mock_response.content = (
            '{"plagiarism_likelihood": "low", "analysis": "Original content"}'
        )
        mock_llm.invoke.return_value = mock_response

        # Mock file operations
        mock_file = Mock()
        mock_open.return_value.__enter__.return_value = mock_file

        state = WorkflowState(
            content="This is original content for testing plagiarism detection.",
            metadata={"name": "Test_Student"},
            source_text="",
            requires_grammar_check=False,
            requires_plagiarism_check=True,
            requires_relevance_check=False,
            requires_grading=False,
            requires_summary=False,
            grammar_result=None,
            plagiarism_result=None,
            relevance_result=None,
            grading_result=None,
            summary_result=None,
            current_step=WorkflowStep.PLAGIARISM_DETECTION.value,
            completed_steps=[],
            errors=[],
            retry_count=0,
            quality_score=0.0,
            final_results=None,
        )

        result_state = plagiarism_detection_agent(state)

        assert result_state["plagiarism_result"]["status"] == "success"
        assert (
            "Test_Student_workflow_report.json"
            in result_state["plagiarism_result"]["report_file"]
        )
        assert result_state["current_step"] == WorkflowStep.RELEVANCE_ANALYSIS.value

    @patch("agentic_workflow.groq_llm")
    def test_content_grading_agent_success(self, mock_llm):
        """Test content grading agent with successful grading."""
        # Mock LLM response with JSON scores
        mock_response = Mock()
        mock_response.content = (
            '{"factuality": 8.5, "relevance": 7.8, "coherence": 8.2, "grammar": 9.0}'
        )
        mock_llm.invoke.return_value = mock_response

        state = WorkflowState(
            content="Well-written assignment about Renaissance art and culture.",
            metadata={"name": "Test Student"},
            source_text="Renaissance source material",
            requires_grammar_check=False,
            requires_plagiarism_check=False,
            requires_relevance_check=False,
            requires_grading=True,
            requires_summary=False,
            grammar_result=None,
            plagiarism_result=None,
            relevance_result=None,
            grading_result=None,
            summary_result=None,
            current_step=WorkflowStep.CONTENT_GRADING.value,
            completed_steps=[],
            errors=[],
            retry_count=0,
            quality_score=0.0,
            final_results=None,
        )

        result_state = content_grading_agent(state)

        assert result_state["grading_result"]["status"] == "success"
        assert result_state["grading_result"]["individual_scores"]["factuality"] == 8.5
        assert result_state["grading_result"]["individual_scores"]["grammar"] == 9.0
        assert result_state["grading_result"]["overall_score"] > 8.0
        assert result_state["grading_result"]["grade_letter"] in ["A+", "A", "A-", "B+"]


class TestWorkflowValidationAndRecovery:
    """Test workflow validation and error recovery mechanisms."""

    def test_quality_validation_agent_success(self):
        """Test quality validation with all successful results."""
        state = WorkflowState(
            content="Test content",
            metadata={"name": "Test Student"},
            source_text="",
            requires_grammar_check=True,
            requires_plagiarism_check=True,
            requires_relevance_check=True,
            requires_grading=True,
            requires_summary=True,
            grammar_result={"status": "success", "error_count": 2},
            plagiarism_result={"status": "success", "report_file": "test.json"},
            relevance_result={"status": "success", "analysis": "Good relevance"},
            grading_result={"status": "success", "overall_score": 8.5},
            summary_result={"status": "success", "summary": "Good summary"},
            current_step=WorkflowStep.QUALITY_VALIDATION.value,
            completed_steps=[],
            errors=[],
            retry_count=0,
            quality_score=0.0,
            final_results=None,
        )

        result_state = quality_validation_agent(state)

        assert result_state["current_step"] == WorkflowStep.RESULTS_AGGREGATION.value
        assert WorkflowStep.QUALITY_VALIDATION.value in result_state["completed_steps"]

    def test_quality_validation_agent_with_failures(self):
        """Test quality validation with some failed results."""
        state = WorkflowState(
            content="Test content",
            metadata={"name": "Test Student"},
            source_text="",
            requires_grammar_check=True,
            requires_plagiarism_check=True,
            requires_relevance_check=False,
            requires_grading=False,
            requires_summary=False,
            grammar_result={"status": "error", "error": "LLM failed"},
            plagiarism_result={"status": "success", "report_file": "test.json"},
            relevance_result=None,
            grading_result=None,
            summary_result=None,
            current_step=WorkflowStep.QUALITY_VALIDATION.value,
            completed_steps=[],
            errors=[],
            retry_count=0,
            quality_score=0.0,
            final_results=None,
        )

        result_state = quality_validation_agent(state)

        # Should trigger error recovery due to failed grammar check
        assert result_state["current_step"] == WorkflowStep.ERROR_RECOVERY.value

    def test_error_recovery_agent(self):
        """Test error recovery mechanism."""
        state = WorkflowState(
            content="Test content",
            metadata={"name": "Test Student"},
            source_text="",
            requires_grammar_check=False,
            requires_plagiarism_check=False,
            requires_relevance_check=False,
            requires_grading=False,
            requires_summary=False,
            grammar_result={"status": "error", "error": "Initial failure"},
            plagiarism_result=None,
            relevance_result=None,
            grading_result=None,
            summary_result=None,
            current_step=WorkflowStep.ERROR_RECOVERY.value,
            completed_steps=[],
            errors=[],
            retry_count=0,
            quality_score=0.0,
            final_results=None,
        )

        result_state = error_recovery_agent(state)

        assert result_state["retry_count"] == 1
        assert result_state["current_step"] == WorkflowStep.RESULTS_AGGREGATION.value
        assert WorkflowStep.ERROR_RECOVERY.value in result_state["completed_steps"]

    def test_results_aggregation_agent(self):
        """Test results aggregation with mixed success/failure results."""
        state = WorkflowState(
            content="Test content",
            metadata={
                "name": "Test Student",
                "date": "2025-01-15",
                "class": "10",
                "subject": "English",
            },
            source_text="",
            requires_grammar_check=False,
            requires_plagiarism_check=False,
            requires_relevance_check=False,
            requires_grading=False,
            requires_summary=False,
            grammar_result={"status": "success", "error_count": 3},
            plagiarism_result={"status": "error", "error": "Analysis failed"},
            relevance_result={"status": "success", "analysis": "Good relevance"},
            grading_result={
                "status": "success",
                "individual_scores": {"factuality": 8.0},
                "overall_score": 8.0,
                "grade_letter": "A-",
            },
            summary_result={"status": "success", "summary": "Test summary"},
            current_step=WorkflowStep.RESULTS_AGGREGATION.value,
            completed_steps=["initialize", "quality_check"],
            errors=["Some error"],
            retry_count=1,
            quality_score=0.8,
            final_results=None,
        )

        result_state = results_aggregation_agent(state)

        assert result_state["final_results"] is not None
        assert result_state["final_results"]["student_name"] == "Test Student"
        assert result_state["final_results"]["grammar_errors"] == 3
        assert result_state["final_results"]["plagiarism_file"] == "Analysis failed"
        assert result_state["final_results"]["content_relevance"] == "Good relevance"
        assert result_state["final_results"]["overall_score"] == 8.0
        assert result_state["final_results"]["letter_grade"] == "A-"
        assert "processing_metadata" in result_state["final_results"]

    def test_finalize_workflow(self):
        """Test workflow finalization."""
        state = WorkflowState(
            content="Test content",
            metadata={"name": "Test Student"},
            source_text="",
            requires_grammar_check=False,
            requires_plagiarism_check=False,
            requires_relevance_check=False,
            requires_grading=False,
            requires_summary=False,
            grammar_result=None,
            plagiarism_result=None,
            relevance_result=None,
            grading_result=None,
            summary_result=None,
            current_step=WorkflowStep.FINALIZE.value,
            completed_steps=["initialize", "quality_check", "results_aggregation"],
            errors=["Test error"],
            retry_count=1,
            quality_score=0.0,
            final_results=None,
        )

        result_state = finalize_workflow(state)

        assert WorkflowStep.FINALIZE.value in result_state["completed_steps"]


class TestFullWorkflowIntegration:
    """Test full workflow integration and end-to-end execution."""

    def test_build_agentic_workflow(self):
        """Test workflow graph construction."""
        workflow = build_agentic_workflow()
        assert workflow is not None

        # Test that the workflow has all required nodes
        # Note: This is a basic test - more detailed graph structure testing
        # would require access to internal LangGraph APIs

    @patch("agentic_workflow.groq_llm")
    async def test_run_agentic_workflow_integration(self, mock_llm):
        """Test full workflow execution with mocked LLM."""
        # Mock LLM responses for different calls
        mock_responses = [
            Mock(content="Grammar analysis: 2 errors found"),
            Mock(content='{"plagiarism": "low risk"}'),
            Mock(content="Relevance: Good alignment with source material"),
            Mock(
                content='{"factuality": 7.5, "relevance": 8.0, "coherence": 7.8, "grammar": 8.5}'
            ),
            Mock(
                content="Summary: This assignment discusses Renaissance art effectively."
            ),
        ]

        mock_llm.invoke.side_effect = mock_responses

        content = "This is a test assignment about Renaissance art and culture. It discusses the major artists and their contributions to the period."
        metadata = {
            "name": "Integration Test Student",
            "date": "2025-01-15",
            "class": "10",
            "subject": "Art History",
        }
        source_text = "The Renaissance was a period of cultural flourishing..."

        with patch("os.makedirs"), patch("builtins.open", create=True):
            result = await run_agentic_workflow(content, metadata, source_text)

        assert result is not None
        assert result["student_name"] == "Integration Test Student"
        assert "workflow_version" in result

    async def test_run_agentic_workflow_with_errors(self):
        """Test workflow execution with simulated errors."""
        content = "Short test"
        metadata = {
            "name": "Error Test Student",
            "date": "2025-01-15",
            "class": "10",
            "subject": "English",
        }
        source_text = ""

        # This should handle errors gracefully and return a fallback result
        result = await run_agentic_workflow(content, metadata, source_text)

        assert result is not None
        assert result["Student Name"] == "Error Test Student"


class TestWorkflowStateManagement:
    """Test workflow state management and transitions."""

    def test_workflow_state_initialization(self):
        """Test that WorkflowState can be properly initialized."""
        state = WorkflowState(
            content="Test content",
            metadata={"name": "Test"},
            source_text="Source",
            requires_grammar_check=True,
            requires_plagiarism_check=False,
            requires_relevance_check=True,
            requires_grading=True,
            requires_summary=False,
            grammar_result=None,
            plagiarism_result=None,
            relevance_result=None,
            grading_result=None,
            summary_result=None,
            current_step="test_step",
            completed_steps=["step1"],
            errors=["error1"],
            retry_count=0,
            quality_score=0.5,
            final_results=None,
        )

        assert state["content"] == "Test content"
        assert state["metadata"]["name"] == "Test"
        assert state["requires_grammar_check"] is True
        assert state["requires_plagiarism_check"] is False
        assert state["quality_score"] == 0.5
        assert state["errors"] == ["error1"]

    def test_workflow_step_enum(self):
        """Test WorkflowStep enum values."""
        assert WorkflowStep.INITIALIZE.value == "initialize"
        assert WorkflowStep.QUALITY_CHECK.value == "quality_check"
        assert WorkflowStep.GRAMMAR_ANALYSIS.value == "grammar_analysis"
        assert WorkflowStep.PLAGIARISM_DETECTION.value == "plagiarism_detection"
        assert WorkflowStep.RELEVANCE_ANALYSIS.value == "relevance_analysis"
        assert WorkflowStep.CONTENT_GRADING.value == "content_grading"
        assert WorkflowStep.SUMMARY_GENERATION.value == "summary_generation"
        assert WorkflowStep.QUALITY_VALIDATION.value == "quality_validation"
        assert WorkflowStep.RESULTS_AGGREGATION.value == "results_aggregation"
        assert WorkflowStep.ERROR_RECOVERY.value == "error_recovery"
        assert WorkflowStep.FINALIZE.value == "finalize"


class TestPerformanceAndStress:
    """Test workflow performance and stress scenarios."""

    async def test_workflow_with_large_content(self):
        """Test workflow with large content input."""
        # Create a large content string
        large_content = "This is a test assignment. " * 1000  # ~25KB of text

        metadata = {
            "name": "Performance Test Student",
            "date": "2025-01-15",
            "class": "10",
            "subject": "English",
        }

        # This should handle large content without issues
        with patch("agentic_workflow.groq_llm") as mock_llm:
            mock_llm.invoke.side_effect = Exception("Simulated LLM failure")

            result = await run_agentic_workflow(large_content, metadata, "")

            assert result is not None
            assert result["Student Name"] == "Performance Test Student"

    def test_workflow_with_empty_content(self):
        """Test workflow behavior with empty or minimal content."""
        state = WorkflowState(
            content="",
            metadata={"name": "Empty Test"},
            source_text="",
            requires_grammar_check=False,
            requires_plagiarism_check=False,
            requires_relevance_check=False,
            requires_grading=False,
            requires_summary=False,
            grammar_result=None,
            plagiarism_result=None,
            relevance_result=None,
            grading_result=None,
            summary_result=None,
            current_step="",
            completed_steps=[],
            errors=[],
            retry_count=0,
            quality_score=0.0,
            final_results=None,
        )

        result_state = initialize_workflow(state)

        # With empty content, most processing should be skipped
        assert result_state["requires_plagiarism_check"] is False  # Content too short
        assert result_state["requires_summary"] is False  # Content too short


if __name__ == "__main__":
    # Run the tests
    print("Running Agentic Workflow Tests...")

    # Create test instances
    component_tests = TestWorkflowComponents()
    mock_tests = TestWorkflowNodesWithMocks()
    validation_tests = TestWorkflowValidationAndRecovery()
    integration_tests = TestFullWorkflowIntegration()
    state_tests = TestWorkflowStateManagement()
    performance_tests = TestPerformanceAndStress()

    # Run individual test methods
    test_methods = [
        # Component tests
        (component_tests.test_letter_grade_conversion, "Letter Grade Conversion"),
        (component_tests.test_initialize_workflow, "Workflow Initialization"),
        (component_tests.test_quality_check_agent, "Quality Check Agent"),
        (component_tests.test_route_workflow, "Workflow Routing"),
        # State tests
        (state_tests.test_workflow_state_initialization, "State Initialization"),
        (state_tests.test_workflow_step_enum, "Workflow Step Enum"),
        # Validation tests
        (
            validation_tests.test_quality_validation_agent_success,
            "Quality Validation Success",
        ),
        (
            validation_tests.test_quality_validation_agent_with_failures,
            "Quality Validation with Failures",
        ),
        (validation_tests.test_error_recovery_agent, "Error Recovery Agent"),
        (validation_tests.test_results_aggregation_agent, "Results Aggregation"),
        (validation_tests.test_finalize_workflow, "Workflow Finalization"),
        # Integration tests
        (integration_tests.test_build_agentic_workflow, "Build Workflow Graph"),
    ]

    # Async test methods
    async_test_methods = [
        (
            integration_tests.test_run_agentic_workflow_integration,
            "Full Workflow Integration",
        ),
        (
            integration_tests.test_run_agentic_workflow_with_errors,
            "Workflow Error Handling",
        ),
        (
            performance_tests.test_workflow_with_large_content,
            "Large Content Performance",
        ),
    ]

    passed = 0
    failed = 0

    # Run synchronous tests
    for test_method, test_name in test_methods:
        try:
            test_method()
            print(f"âœ… {test_name}")
            passed += 1
        except Exception as e:
            print(f"âŒ {test_name}: {str(e)}")
            failed += 1

    # Run asynchronous tests
    async def run_async_tests():
        global passed, failed
        for test_method, test_name in async_test_methods:
            try:
                await test_method()
                print(f"âœ… {test_name}")
                passed += 1
            except Exception as e:
                print(f"âŒ {test_name}: {str(e)}")
                failed += 1

    asyncio.run(run_async_tests())

    # Run mock-based tests
    try:
        mock_tests.test_grammar_analysis_agent_success()
        print("âœ… Grammar Analysis with Mock")
        passed += 1
    except Exception as e:
        print(f"âŒ Grammar Analysis with Mock: {str(e)}")
        failed += 1

    try:
        mock_tests.test_grammar_analysis_agent_no_llm()
        print("âœ… Grammar Analysis No LLM")
        passed += 1
    except Exception as e:
        print(f"âŒ Grammar Analysis No LLM: {str(e)}")
        failed += 1

    try:
        mock_tests.test_plagiarism_detection_agent_success()
        print("âœ… Plagiarism Detection with Mock")
        passed += 1
    except Exception as e:
        print(f"âŒ Plagiarism Detection with Mock: {str(e)}")
        failed += 1

    try:
        mock_tests.test_content_grading_agent_success()
        print("âœ… Content Grading with Mock")
        passed += 1
    except Exception as e:
        print(f"âŒ Content Grading with Mock: {str(e)}")
        failed += 1

    try:
        performance_tests.test_workflow_with_empty_content()
        print("âœ… Empty Content Handling")
        passed += 1
    except Exception as e:
        print(f"âŒ Empty Content Handling: {str(e)}")
        failed += 1

    # Print summary
    print("\n" + "=" * 50)
    print("TEST SUMMARY")
    print("=" * 50)
    print(f"Passed: {passed}")
    print(f"Failed: {failed}")
    print(f"Total: {passed + failed}")
    print(
        f"Success Rate: {(passed / (passed + failed) * 100):.1f}%"
        if (passed + failed) > 0
        else "No tests run"
    )

    if failed == 0:
        print("ğŸ‰ All tests passed!")
    else:
        print(f"âš ï¸ {failed} test(s) failed. Please review the failures above.")
