# LLM provider configuration for RT-Capstone backend

# Order in which providers are attempted when no explicit provider is chosen
provider_priority:
  1: groq
  2: openai
  3: anthropic
  4: gemini

# Providers: set enabled: true only for those you have API keys for
providers:
  groq:
    enabled: true  # set true if GROQ_API_KEY is set
    models:
      default: llama-3.1-8b-instant
    temperature: 0.7
    max_tokens: 4096
    timeout: 60

  openai:
    enabled: false  # set true if OPENAI_API_KEY is set
    models:
      default: gpt-4o-mini
    temperature: 0.7
    max_tokens: 4096
    timeout: 60

  anthropic:
    enabled: false  # set true if ANTHROPIC_API_KEY is set
    models:
      default: claude-3-5-sonnet-20241022
    temperature: 0.7
    max_tokens: 4096
    timeout: 60

  gemini:
    enabled: false  # set true if GEMINI_API_KEY is set
    models:
      default: gemini-1.5-pro
    temperature: 0.7
    max_tokens: 4096
    timeout: 60

# Failover / circuit breaker settings
failover:
  circuit_breaker_threshold: 5
  circuit_breaker_timeout: 300
